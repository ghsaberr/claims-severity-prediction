{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc065d4a-d06d-4f22-8c35-8e1b261be526",
   "metadata": {},
   "source": [
    "# Claims Severity Prediction by Fine-Tuning a Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba838693-6183-4994-9b78-b5a419c17a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASIC PANDAS PROFILING FOR NUMERIC COLUMNS ===\n",
      "Dataset shape: (54000, 15)\n",
      "\n",
      "Numeric columns found: 8\n",
      "Column names: ['Age', 'DependentChildren', 'DependentsOther', 'WeeklyWages', 'HoursWorkedPerWeek', 'DaysWorkedPerWeek', 'InitialIncurredCalimsCost', 'UltimateIncurredClaimCost']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š COLUMN: Age\n",
      "--------------------------------------------------\n",
      "ðŸ“ˆ Descriptive Statistics:\n",
      "count    54000.000000\n",
      "mean        33.842370\n",
      "std         12.122165\n",
      "min         13.000000\n",
      "25%         23.000000\n",
      "50%         32.000000\n",
      "75%         43.000000\n",
      "max         81.000000\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "ðŸ” Data Quality:\n",
      "   â€¢ Null values: 0 (0.00%)\n",
      "   â€¢ Unique values: 68\n",
      "   â€¢ Data type: int64\n",
      "   â€¢ Range: 68.00\n",
      "   â€¢ Coefficient of Variation: 35.82%\n",
      "   â€¢ Potential outliers: 22 (0.04%)\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š COLUMN: DependentChildren\n",
      "--------------------------------------------------\n",
      "ðŸ“ˆ Descriptive Statistics:\n",
      "count    54000.000000\n",
      "mean         0.119185\n",
      "std          0.517780\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          9.000000\n",
      "Name: DependentChildren, dtype: float64\n",
      "\n",
      "ðŸ” Data Quality:\n",
      "   â€¢ Null values: 0 (0.00%)\n",
      "   â€¢ Unique values: 9\n",
      "   â€¢ Data type: int64\n",
      "   â€¢ Range: 9.00\n",
      "   â€¢ Coefficient of Variation: 434.43%\n",
      "   â€¢ Potential outliers: 3361 (6.22%)\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š COLUMN: DependentsOther\n",
      "--------------------------------------------------\n",
      "ðŸ“ˆ Descriptive Statistics:\n",
      "count    54000.000000\n",
      "mean         0.009944\n",
      "std          0.109348\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          5.000000\n",
      "Name: DependentsOther, dtype: float64\n",
      "\n",
      "ðŸ” Data Quality:\n",
      "   â€¢ Null values: 0 (0.00%)\n",
      "   â€¢ Unique values: 5\n",
      "   â€¢ Data type: int64\n",
      "   â€¢ Range: 5.00\n",
      "   â€¢ Coefficient of Variation: 1099.58%\n",
      "   â€¢ Potential outliers: 494 (0.91%)\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š COLUMN: WeeklyWages\n",
      "--------------------------------------------------\n",
      "ðŸ“ˆ Descriptive Statistics:\n",
      "count    54000.000000\n",
      "mean       416.364807\n",
      "std        248.638669\n",
      "min          1.000000\n",
      "25%        200.000000\n",
      "50%        392.200000\n",
      "75%        500.000000\n",
      "max       7497.000000\n",
      "Name: WeeklyWages, dtype: float64\n",
      "\n",
      "ðŸ” Data Quality:\n",
      "   â€¢ Null values: 0 (0.00%)\n",
      "   â€¢ Unique values: 13211\n",
      "   â€¢ Data type: float64\n",
      "   â€¢ Range: 7496.00\n",
      "   â€¢ Coefficient of Variation: 59.72%\n",
      "   â€¢ Potential outliers: 1478 (2.74%)\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š COLUMN: HoursWorkedPerWeek\n",
      "--------------------------------------------------\n",
      "ðŸ“ˆ Descriptive Statistics:\n",
      "count    54000.000000\n",
      "mean        37.735084\n",
      "std         12.568704\n",
      "min          0.000000\n",
      "25%         38.000000\n",
      "50%         38.000000\n",
      "75%         40.000000\n",
      "max        640.000000\n",
      "Name: HoursWorkedPerWeek, dtype: float64\n",
      "\n",
      "ðŸ” Data Quality:\n",
      "   â€¢ Null values: 0 (0.00%)\n",
      "   â€¢ Unique values: 424\n",
      "   â€¢ Data type: float64\n",
      "   â€¢ Range: 640.00\n",
      "   â€¢ Coefficient of Variation: 33.31%\n",
      "   â€¢ Potential outliers: 7446 (13.79%)\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š COLUMN: DaysWorkedPerWeek\n",
      "--------------------------------------------------\n",
      "ðŸ“ˆ Descriptive Statistics:\n",
      "count    54000.000000\n",
      "mean         4.905759\n",
      "std          0.552129\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          5.000000\n",
      "75%          5.000000\n",
      "max          7.000000\n",
      "Name: DaysWorkedPerWeek, dtype: float64\n",
      "\n",
      "ðŸ” Data Quality:\n",
      "   â€¢ Null values: 0 (0.00%)\n",
      "   â€¢ Unique values: 7\n",
      "   â€¢ Data type: int64\n",
      "   â€¢ Range: 6.00\n",
      "   â€¢ Coefficient of Variation: 11.25%\n",
      "   â€¢ Potential outliers: 4815 (8.92%)\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š COLUMN: InitialIncurredCalimsCost\n",
      "--------------------------------------------------\n",
      "ðŸ“ˆ Descriptive Statistics:\n",
      "count    5.400000e+04\n",
      "mean     7.841146e+03\n",
      "std      2.058408e+04\n",
      "min      1.000000e+00\n",
      "25%      7.000000e+02\n",
      "50%      2.000000e+03\n",
      "75%      9.500000e+03\n",
      "max      2.000000e+06\n",
      "Name: InitialIncurredCalimsCost, dtype: float64\n",
      "\n",
      "ðŸ” Data Quality:\n",
      "   â€¢ Null values: 0 (0.00%)\n",
      "   â€¢ Unique values: 1989\n",
      "   â€¢ Data type: int64\n",
      "   â€¢ Range: 1999999.00\n",
      "   â€¢ Coefficient of Variation: 262.51%\n",
      "   â€¢ Potential outliers: 4355 (8.06%)\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š COLUMN: UltimateIncurredClaimCost\n",
      "--------------------------------------------------\n",
      "ðŸ“ˆ Descriptive Statistics:\n",
      "count    5.400000e+04\n",
      "mean     1.100337e+04\n",
      "std      3.339099e+04\n",
      "min      1.218868e+02\n",
      "25%      9.263384e+02\n",
      "50%      3.371242e+03\n",
      "75%      8.197249e+03\n",
      "max      4.027136e+06\n",
      "Name: UltimateIncurredClaimCost, dtype: float64\n",
      "\n",
      "ðŸ” Data Quality:\n",
      "   â€¢ Null values: 0 (0.00%)\n",
      "   â€¢ Unique values: 53999\n",
      "   â€¢ Data type: float64\n",
      "   â€¢ Range: 4027014.05\n",
      "   â€¢ Coefficient of Variation: 303.46%\n",
      "   â€¢ Potential outliers: 6805 (12.60%)\n",
      "==================================================\n",
      "\n",
      "âœ… Profiling completed for 8 numeric columns!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "print(\"=== BASIC PANDAS PROFILING FOR NUMERIC COLUMNS ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns\n",
    "print(f\"\\nNumeric columns found: {len(numeric_columns)}\")\n",
    "print(f\"Column names: {numeric_columns.tolist()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Profile each numeric column\n",
    "for col in numeric_columns:\n",
    "    print(f\"\\nðŸ“Š COLUMN: {col}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"ðŸ“ˆ Descriptive Statistics:\")\n",
    "    print(df[col].describe())\n",
    "    \n",
    "    # Data quality\n",
    "    print(f\"\\nðŸ” Data Quality:\")\n",
    "    print(f\"   â€¢ Null values: {df[col].isnull().sum()} ({df[col].isnull().sum()/len(df)*100:.2f}%)\")\n",
    "    print(f\"   â€¢ Unique values: {df[col].nunique()}\")\n",
    "    print(f\"   â€¢ Data type: {df[col].dtype}\")\n",
    "    \n",
    "    # Additional insights\n",
    "    if df[col].nunique() > 1:  # Avoid division by zero\n",
    "        print(f\"   â€¢ Range: {df[col].max() - df[col].min():.2f}\")\n",
    "        print(f\"   â€¢ Coefficient of Variation: {df[col].std()/df[col].mean()*100:.2f}%\")\n",
    "    \n",
    "    # Check for potential outliers (using IQR method)\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    print(f\"   â€¢ Potential outliers: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nâœ… Profiling completed for {len(numeric_columns)} numeric columns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa8b7d-08b0-4080-9a06-6dff61636fc9",
   "metadata": {},
   "source": [
    "## Numerical Columns Profiling Analysis\n",
    "\n",
    "### Dataset Overview\n",
    "- **Total Records:** 54,000 \n",
    "- **Numerical Columns:** 8 columns\n",
    "- **Data Quality:** Excellent - Zero missing values across all features\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Analysis of All Numerical Features\n",
    "\n",
    "#### **Demographic Features**\n",
    "\n",
    "**1. Age**\n",
    "- **Mean:** 33.8 years | **Range:** 13-81 years (68 years span)\n",
    "- **Distribution:** Well-balanced (CV: 35.82%)\n",
    "- **Quality:** Minimal outliers (0.04%) - excellent for modeling\n",
    "\n",
    "**2. DependentChildren**\n",
    "- **Mean:** 0.12 children | **Range:** 0-9 children\n",
    "- **Distribution:** Highly skewed - 75% have no children (CV: 434%)\n",
    "- **Outliers:** 6.22% - natural for count data pattern\n",
    "\n",
    "**3. DependentsOther**\n",
    "- **Mean:** 0.01 individuals | **Range:** 0-5 dependents  \n",
    "- **Distribution:** Extremely sparse (CV: 1099%) - 99% have zero\n",
    "- **Recommendation:** Consider removal due to low information value\n",
    "\n",
    "---\n",
    "\n",
    "#### **Employment & Economic Features**\n",
    "\n",
    "**4. WeeklyWages**\n",
    "- **Mean:** $416.36 | **Median:** $392.20 | **Range:** $1-$7,497\n",
    "- **Distribution:** Right-skewed (CV: 59.72%) - typical wage distribution\n",
    "- **Outliers:** 2.74% - high earners, manageable level\n",
    "- **Insights:** Most workers earn $200-$500/week (Q1-Q3)\n",
    "\n",
    "**5. HoursWorkedPerWeek** âš ï¸\n",
    "- **Mean:** 37.7 hours | **Median:** 38 hours | **Range:** 0-640 hours\n",
    "- **Distribution:** Clustered around full-time (CV: 33.31%)\n",
    "- **Critical Issue:** 13.79% outliers - some unrealistic values (640+ hours)\n",
    "- **Action Required:** Cap extreme values or investigate data quality\n",
    "\n",
    "**6. DaysWorkedPerWeek**\n",
    "- **Mean:** 4.9 days | **Median:** 5 days | **Range:** 1-7 days\n",
    "- **Distribution:** Very stable (CV: 11.25%) - mostly standard work week\n",
    "- **Outliers:** 8.92% - likely weekend/shift workers\n",
    "\n",
    "---\n",
    "\n",
    "#### **Claim Cost Features (Critical for Prediction)**\n",
    "\n",
    "**7. InitialIncurredClaimsCost**\n",
    "- **Mean:** $7,841 | **Median:** $2,000 | **Range:** $1-$2M\n",
    "- **Distribution:** Heavily right-skewed (CV: 262.51%)\n",
    "- **Outliers:** 8.06% - high-cost initial assessments\n",
    "- **Pattern:** Mean >> Median indicates extreme skewness\n",
    "\n",
    "**8. UltimateIncurredClaimCost** **[TARGET VARIABLE]**\n",
    "- **Mean:** $11,003 | **Median:** $3,371 | **Range:** $122-$4.03M\n",
    "- **Distribution:** Extremely right-skewed (CV: 303.46%)\n",
    "- **Outliers:** 12.60% - highest among all features\n",
    "- **Critical Insight:** Claims escalate from initial ($7.8K) to ultimate ($11K) on average\n",
    "\n",
    "---\n",
    "\n",
    "### Key Data Insights\n",
    "\n",
    "#### **Distribution Patterns:**\n",
    "- **Normal/Balanced:** Age, DaysWorkedPerWeek\n",
    "- **Right-Skewed:** WeeklyWages, Cost variables\n",
    "- **Highly Sparse:** DependentChildren, DependentsOther\n",
    "- **Clustered:** HoursWorkedPerWeek around 38-40 hours\n",
    "\n",
    "#### **Data Quality Issues:**\n",
    "1. **HoursWorkedPerWeek:** Unrealistic maximum (640 hours) needs investigation\n",
    "2. **Cost Variables:** Extreme outliers but expected in insurance data\n",
    "3. **Dependent Variables:** Very sparse, limited predictive value\n",
    "\n",
    "---\n",
    "\n",
    "### **Comprehensive Preprocessing Strategy**\n",
    "\n",
    "#### **Feature Transformations:**\n",
    "\n",
    "**1. Log Transformation Required:**\n",
    "- WeeklyWages, InitialIncurredClaimsCost, UltimateIncurredClaimCost\n",
    "- *Reason:* Heavy right-skewness (CV > 100%)\n",
    "\n",
    "**2. Outlier Treatment:**\n",
    "- **HoursWorkedPerWeek:** Cap at reasonable maximum (80 hours)\n",
    "- **Cost Variables:** Use robust scaling methods\n",
    "- **Age:** Minimal outliers, keep as-is\n",
    "\n",
    "**3. Feature Engineering:**\n",
    "- **Hourly Rate:** WeeklyWages Ã· HoursWorkedPerWeek\n",
    "- **Cost Escalation:** UltimateIncurredClaimCost Ã· InitialIncurredClaimsCost\n",
    "- **Work Intensity:** Categorical encoding for hours/days patterns\n",
    "\n",
    "**4. Scaling Strategy:**\n",
    "- **StandardScaler:** Age, work pattern features\n",
    "- **RobustScaler:** Cost variables (outlier-resistant)\n",
    "- **LogNormal:** Wage and cost variables after log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173aaa19-b4e5-4b2c-a84c-d3f19fe890f8",
   "metadata": {},
   "source": [
    "### Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89faede6-0f17-44c1-984f-35b924e68275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA PREPROCESSING: OUTLIER TREATMENT & SCALING ===\n",
      "Original dataset shape: (54000, 15)\n",
      "\n",
      "ðŸ• HOURS WORKED PER WEEK - OUTLIER TREATMENT\n",
      "------------------------------------------------------------\n",
      "Original HoursWorkedPerWeek statistics:\n",
      "count    54000.000000\n",
      "mean        37.735084\n",
      "std         12.568704\n",
      "min          0.000000\n",
      "25%         38.000000\n",
      "50%         38.000000\n",
      "75%         40.000000\n",
      "max        640.000000\n",
      "Name: HoursWorkedPerWeek, dtype: float64\n",
      "\n",
      "Records with >100 hours: 37 (0.07%)\n",
      "Values >100 hours: [np.float64(168.0), np.float64(250.0), np.float64(319.8), np.float64(320.0), np.float64(338.0), np.float64(350.0), np.float64(360.0), np.float64(380.0), np.float64(383.0), np.float64(385.0), np.float64(387.0), np.float64(389.0), np.float64(400.0), np.float64(410.0), np.float64(417.2), np.float64(450.0), np.float64(462.08), np.float64(538.3), np.float64(627.0), np.float64(638.0), np.float64(640.0)]\n",
      "\n",
      "Examples of extreme hours:\n",
      "  250.0 hours, $338/week: WELDING FLASH FROM WELDER SORE EYES...\n",
      "  410.0 hours, $585/week: LIFTING PALLETS NECK STRAIN...\n",
      "  168.0 hours, $200/week: REPETITIVE USE ON KEYBOARD STRAINED RIGHT KNEE...\n",
      "  389.0 hours, $200/week: PULLING PALLET BACKWARDS STRAIN LOWER BACK STRAIN...\n",
      "  417.2 hours, $456/week: SLICING MEAT IN MEAT SLICER LACERATED LEFT HAND...\n",
      "\n",
      "After capping at 100 hours:\n",
      "  Records modified: 37\n",
      "  New max hours: 100.0\n",
      "  New statistics:\n",
      "count    54000.000000\n",
      "mean        37.514593\n",
      "std          7.195841\n",
      "min          0.000000\n",
      "25%         38.000000\n",
      "50%         38.000000\n",
      "75%         40.000000\n",
      "max        100.000000\n",
      "Name: HoursWorkedPerWeek, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "print(\"=== DATA PREPROCESSING: OUTLIER TREATMENT & SCALING ===\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# HOURS WORKED PER WEEK - CAPPING AT 100 HOURS\n",
    "print(\"\\nðŸ• HOURS WORKED PER WEEK - OUTLIER TREATMENT\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "hours_col = 'HoursWorkedPerWeek'\n",
    "print(f\"Original {hours_col} statistics:\")\n",
    "print(df[hours_col].describe())\n",
    "\n",
    "# Check extreme values\n",
    "extreme_hours = df[df[hours_col] > 100]\n",
    "print(f\"\\nRecords with >100 hours: {len(extreme_hours)} ({len(extreme_hours)/len(df)*100:.2f}%)\")\n",
    "print(f\"Values >100 hours: {sorted(extreme_hours[hours_col].unique())}\")\n",
    "\n",
    "# Show some extreme examples\n",
    "if len(extreme_hours) > 0:\n",
    "    print(\"\\nExamples of extreme hours:\")\n",
    "    sample_extreme = extreme_hours.head(5)[[hours_col, 'WeeklyWages', 'ClaimDescription']]\n",
    "    for idx, row in sample_extreme.iterrows():\n",
    "        print(f\"  {row[hours_col]} hours, ${row['WeeklyWages']:.0f}/week: {row['ClaimDescription'][:50]}...\")\n",
    "\n",
    "# Apply capping\n",
    "df_processed = df.copy()\n",
    "df_processed[hours_col] = df_processed[hours_col].clip(upper=100)\n",
    "\n",
    "print(f\"\\nAfter capping at 100 hours:\")\n",
    "print(f\"  Records modified: {(df[hours_col] > 100).sum()}\")\n",
    "print(f\"  New max hours: {df_processed[hours_col].max()}\")\n",
    "print(f\"  New statistics:\")\n",
    "print(df_processed[hours_col].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58e79a-b904-488d-a7c6-d8e15dda48d4",
   "metadata": {},
   "source": [
    "### Transformation and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7eb233d-b76c-403e-b29f-409314c82228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’° COST VARIABLES - ROBUST SCALING\n",
      "------------------------------------------------------------\n",
      "Original cost variable distributions:\n",
      "\n",
      "InitialIncurredCalimsCost:\n",
      "  Mean: $7,841.15\n",
      "  Median: $2,000.00\n",
      "  Std: $20,584.08\n",
      "  Min: $1.00\n",
      "  Max: $2,000,000.00\n",
      "  Skewness: 26.85\n",
      "  Values above 99th percentile ($75,000): 534 (0.99%)\n",
      "\n",
      "UltimateIncurredClaimCost:\n",
      "  Mean: $11,003.37\n",
      "  Median: $3,371.24\n",
      "  Std: $33,390.99\n",
      "  Min: $121.89\n",
      "  Max: $4,027,135.94\n",
      "  Skewness: 37.55\n",
      "  Values above 99th percentile ($139,025): 540 (1.00%)\n",
      "\n",
      "Applying Log Transformation to: ['WeeklyWages', 'InitialIncurredCalimsCost', 'UltimateIncurredClaimCost']\n",
      "\n",
      "WeeklyWages â†’ WeeklyWages_Log:\n",
      "  Original skewness: 4.12\n",
      "  Log-transformed skewness: -2.09\n",
      "  Skewness improvement: 2.03\n",
      "\n",
      "InitialIncurredCalimsCost â†’ InitialIncurredCalimsCost_Log:\n",
      "  Original skewness: 26.85\n",
      "  Log-transformed skewness: 0.21\n",
      "  Skewness improvement: 26.64\n",
      "\n",
      "UltimateIncurredClaimCost â†’ UltimateIncurredClaimCost_Log:\n",
      "  Original skewness: 37.55\n",
      "  Log-transformed skewness: 0.29\n",
      "  Skewness improvement: 37.27\n",
      "\n",
      "Robust Scaling Results:\n",
      "Robust Scaler parameters:\n",
      "  Median (center): [7.60140233 8.123333  ]\n",
      "  IQR (scale): [2.60664445 2.17935723]\n",
      "\n",
      "Log-transformed and scaled cost variables statistics:\n",
      "\n",
      "InitialIncurredCalimsCost_LogRobustScaled:\n",
      "  Mean: 0.0859\n",
      "  Median: 0.0000\n",
      "  Std: 0.5789\n",
      "  Min: -2.6502\n",
      "  Max: 2.6499\n",
      "\n",
      "UltimateIncurredClaimCost_LogRobustScaled:\n",
      "  Mean: -0.0268\n",
      "  Median: 0.0000\n",
      "  Std: 0.7001\n",
      "  Min: -1.5197\n",
      "  Max: 3.2511\n",
      "\n",
      "WeeklyWages_Log statistics:\n",
      "count    54000.000000\n",
      "mean         5.871715\n",
      "std          0.638201\n",
      "min          0.693147\n",
      "25%          5.303305\n",
      "50%          5.974318\n",
      "75%          6.216606\n",
      "max          8.922392\n",
      "Name: WeeklyWages_Log, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# COST VARIABLES - ROBUST SCALING\n",
    "print(\"\\nðŸ’° COST VARIABLES - ROBUST SCALING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "cost_columns = ['InitialIncurredCalimsCost', 'UltimateIncurredClaimCost']\n",
    "\n",
    "# Analyze original distributions\n",
    "print(\"Original cost variable distributions:\")\n",
    "for col in cost_columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Mean: ${df[col].mean():,.2f}\")\n",
    "    print(f\"  Median: ${df[col].median():,.2f}\")\n",
    "    print(f\"  Std: ${df[col].std():,.2f}\")\n",
    "    print(f\"  Min: ${df[col].min():,.2f}\")\n",
    "    print(f\"  Max: ${df[col].max():,.2f}\")\n",
    "    print(f\"  Skewness: {df[col].skew():.2f}\")\n",
    "    \n",
    "    # Count of extreme outliers (>99th percentile)\n",
    "    p99 = df[col].quantile(0.99)\n",
    "    extreme_count = (df[col] > p99).sum()\n",
    "    print(f\"  Values above 99th percentile (${p99:,.0f}): {extreme_count} ({extreme_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Apply Log Transformation first (for right-skewed variables)\n",
    "log_transform_columns = ['WeeklyWages'] + cost_columns\n",
    "print(f\"\\nApplying Log Transformation to: {log_transform_columns}\")\n",
    "\n",
    "for col in log_transform_columns:\n",
    "    # Add small constant to avoid log(0)\n",
    "    df_processed[f'{col}_Log'] = np.log1p(df[col])  # log1p = log(1+x)\n",
    "    \n",
    "    print(f\"\\n{col} â†’ {col}_Log:\")\n",
    "    print(f\"  Original skewness: {df[col].skew():.2f}\")\n",
    "    print(f\"  Log-transformed skewness: {df_processed[f'{col}_Log'].skew():.2f}\")\n",
    "    print(f\"  Skewness improvement: {abs(df[col].skew()) - abs(df_processed[f'{col}_Log'].skew()):.2f}\")\n",
    "\n",
    "# Apply robust scaling to log-transformed cost variables\n",
    "scaler = RobustScaler()\n",
    "log_cost_columns = [f'{col}_Log' for col in cost_columns]\n",
    "cost_log_data = df_processed[log_cost_columns].values\n",
    "cost_log_scaled = scaler.fit_transform(cost_log_data)\n",
    "\n",
    "# Add scaled columns to dataframe\n",
    "for i, col in enumerate(cost_columns):\n",
    "    df_processed[f'{col}_LogRobustScaled'] = cost_log_scaled[:, i]\n",
    "\n",
    "print(f\"\\nRobust Scaling Results:\")\n",
    "print(\"Robust Scaler parameters:\")\n",
    "print(f\"  Median (center): {scaler.center_}\")\n",
    "print(f\"  IQR (scale): {scaler.scale_}\")\n",
    "\n",
    "print(f\"\\nLog-transformed and scaled cost variables statistics:\")\n",
    "for i, col in enumerate(cost_columns):\n",
    "    scaled_col = f'{col}_LogRobustScaled'\n",
    "    print(f\"\\n{scaled_col}:\")\n",
    "    print(f\"  Mean: {df_processed[scaled_col].mean():.4f}\")\n",
    "    print(f\"  Median: {df_processed[scaled_col].median():.4f}\")\n",
    "    print(f\"  Std: {df_processed[scaled_col].std():.4f}\")\n",
    "    print(f\"  Min: {df_processed[scaled_col].min():.4f}\")\n",
    "    print(f\"  Max: {df_processed[scaled_col].max():.4f}\")\n",
    "\n",
    "# Also show WeeklyWages log transformation stats\n",
    "print(f\"\\nWeeklyWages_Log statistics:\")\n",
    "wages_log_stats = df_processed['WeeklyWages_Log'].describe()\n",
    "print(wages_log_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9029d-0acb-425d-886a-59a79e169661",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5603e7-4a4d-4d07-a709-1aefe0c56cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " FEATURE ENGINEERING: DERIVED FEATURES\n",
      "------------------------------------------------------------\n",
      "New feature: HourlyWage\n",
      "count    53971.000000\n",
      "mean        11.312875\n",
      "std          7.839644\n",
      "min          0.014286\n",
      "25%          5.714286\n",
      "50%         10.560526\n",
      "75%         13.611539\n",
      "max        519.200000\n",
      "Name: HourlyWage, dtype: float64\n",
      "HourlyWage skewness: 13.27\n",
      "HourlyWage_Log skewness: -0.39\n",
      "\n",
      "New feature: CostEscalationRatio\n",
      "count    54000.000000\n",
      "mean         4.986533\n",
      "std        113.925455\n",
      "min          0.043632\n",
      "25%          0.751631\n",
      "50%          1.063106\n",
      "75%          1.806784\n",
      "max       8763.805521\n",
      "Name: CostEscalationRatio, dtype: float64\n",
      "\n",
      "New feature: WorkIntensityCategory\n",
      "WorkIntensityCategory\n",
      "Standard     48989\n",
      "Part-time     2543\n",
      "Extended      1430\n",
      "Intensive     1038\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# FEATURE ENGINEERING: ADDITIONAL DERIVED FEATURES\n",
    "print(\"\\n FEATURE ENGINEERING: DERIVED FEATURES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Hourly wage calculation (with safe division)\n",
    "df_processed['HourlyWage'] = df_processed['WeeklyWages'] / df_processed[hours_col].replace(0, np.nan)\n",
    "# Also create log-transformed hourly wage\n",
    "df_processed['HourlyWage_Log'] = np.log1p(df_processed['HourlyWage'].fillna(0))\n",
    "\n",
    "hourly_wage_stats = df_processed['HourlyWage'].describe()\n",
    "print(\"New feature: HourlyWage\")\n",
    "print(hourly_wage_stats)\n",
    "print(f\"HourlyWage skewness: {df_processed['HourlyWage'].skew():.2f}\")\n",
    "print(f\"HourlyWage_Log skewness: {df_processed['HourlyWage_Log'].skew():.2f}\")\n",
    "\n",
    "# Cost escalation ratio\n",
    "df_processed['CostEscalationRatio'] = (df_processed['UltimateIncurredClaimCost'] / \n",
    "                                      df_processed['InitialIncurredCalimsCost'].replace(0, np.nan))\n",
    "escalation_stats = df_processed['CostEscalationRatio'].describe()\n",
    "print(f\"\\nNew feature: CostEscalationRatio\")\n",
    "print(escalation_stats)\n",
    "\n",
    "# Work intensity categorization\n",
    "def categorize_work_intensity(hours):\n",
    "    if hours <= 20:\n",
    "        return 'Part-time'\n",
    "    elif hours <= 40:\n",
    "        return 'Standard'\n",
    "    elif hours <= 50:\n",
    "        return 'Extended'\n",
    "    else:\n",
    "        return 'Intensive'\n",
    "\n",
    "df_processed['WorkIntensityCategory'] = df_processed[hours_col].apply(categorize_work_intensity)\n",
    "work_intensity_dist = df_processed['WorkIntensityCategory'].value_counts()\n",
    "print(f\"\\nNew feature: WorkIntensityCategory\")\n",
    "print(work_intensity_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226576fc-be87-4476-984b-62c290b39655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SAVING PROCESSED DATA\n",
      "------------------------------------------------------------\n",
      "Processed training data saved to: train_processed.csv\n",
      "New dataset shape: (54000, 24)\n",
      "New columns added: 9\n",
      "\n",
      "New columns created:\n",
      "  â€¢ WeeklyWages_Log\n",
      "  â€¢ InitialIncurredCalimsCost_Log\n",
      "  â€¢ UltimateIncurredClaimCost_Log\n",
      "  â€¢ InitialIncurredCalimsCost_LogRobustScaled\n",
      "  â€¢ UltimateIncurredClaimCost_LogRobustScaled\n",
      "  â€¢ HourlyWage\n",
      "  â€¢ HourlyWage_Log\n",
      "  â€¢ CostEscalationRatio\n",
      "  â€¢ WorkIntensityCategory\n"
     ]
    }
   ],
   "source": [
    "# SAVE PROCESSED DATA\n",
    "print(\"\\n SAVING PROCESSED DATA\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Save the processed training data\n",
    "output_file = 'train_processed.csv'\n",
    "df_processed.to_csv(output_file, index=False)\n",
    "print(f\"Processed training data saved to: {output_file}\")\n",
    "print(f\"New dataset shape: {df_processed.shape}\")\n",
    "print(f\"New columns added: {df_processed.shape[1] - df.shape[1]}\")\n",
    "\n",
    "print(\"\\nNew columns created:\")\n",
    "new_columns = [col for col in df_processed.columns if col not in df.columns]\n",
    "for col in new_columns:\n",
    "    print(f\"  â€¢ {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360924b-1ff0-4287-a354-f7c75a4779ce",
   "metadata": {},
   "source": [
    "## NLP-specific stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5a51fc4-58d2-46a6-8897-f9804dde9e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NLP-SPECIFIC STATISTICS FOR CLAIM DESCRIPTIONS ===\n",
      "Total descriptions: 54000\n",
      "Empty descriptions: 0\n",
      "\n",
      " TOKEN LENGTH ANALYSIS\n",
      "--------------------------------------------------\n",
      "Average tokens per description: 7.02\n",
      "Median tokens: 7.0\n",
      "Min tokens: 1\n",
      "Max tokens: 14\n",
      "Standard deviation: 1.65\n",
      "\n",
      "Average characters per description: 43.45\n",
      "Max characters: 94\n",
      "\n",
      " TOP N-GRAMS ANALYSIS\n",
      "--------------------------------------------------\n",
      "Top 15 Words (1-grams):\n",
      "  'RIGHT': 22648 times (41.94% of descriptions)\n",
      "  'LEFT': 20756 times (38.44% of descriptions)\n",
      "  'BACK': 16346 times (30.27% of descriptions)\n",
      "  'STRAIN': 15259 times (28.26% of descriptions)\n",
      "  'LOWER': 9950 times (18.43% of descriptions)\n",
      "  'AND': 9103 times (16.86% of descriptions)\n",
      "  'FINGER': 8584 times (15.90% of descriptions)\n",
      "  'LIFTING': 8300 times (15.37% of descriptions)\n",
      "  'HAND': 7723 times (14.30% of descriptions)\n",
      "  'STRUCK': 7354 times (13.62% of descriptions)\n",
      "  'SHOULDER': 6198 times (11.48% of descriptions)\n",
      "  'FELL': 5747 times (10.64% of descriptions)\n",
      "  'SLIPPED': 5640 times (10.44% of descriptions)\n",
      "  'LACERATION': 5480 times (10.15% of descriptions)\n",
      "  'EYE': 5343 times (9.89% of descriptions)\n",
      "\n",
      "Top 10 2-grams:\n",
      "  'lower back': 9442 times\n",
      "  'back strain': 6374 times\n",
      "  'foreign body': 3880 times\n",
      "  'strain lower': 2981 times\n",
      "  'left hand': 2747 times\n",
      "  'right shoulder': 2609 times\n",
      "  'index finger': 2571 times\n",
      "  'right hand': 2555 times\n",
      "  'laceration left': 2333 times\n",
      "  'right knee': 2193 times\n",
      "\n",
      " VOCABULARY ANALYSIS\n",
      "--------------------------------------------------\n",
      "Total unique words (vocabulary size): 3726\n",
      "Total word instances: 379048\n",
      "Vocabulary richness: 0.0098\n",
      "\n",
      "Domain-specific word categories:\n",
      "  Injury-related words: 7336 instances\n",
      "  Body part mentions: 50753 instances\n",
      "  Action words: 24771 instances\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "descriptions = df['ClaimDescription'].fillna('')\n",
    "\n",
    "print(\"=== NLP-SPECIFIC STATISTICS FOR CLAIM DESCRIPTIONS ===\")\n",
    "print(f\"Total descriptions: {len(descriptions)}\")\n",
    "print(f\"Empty descriptions: {descriptions.str.strip().eq('').sum()}\")\n",
    "\n",
    "# TOKEN LENGTH ANALYSIS\n",
    "print(\"\\n TOKEN LENGTH ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "token_lengths = descriptions.str.split().str.len()\n",
    "print(f\"Average tokens per description: {token_lengths.mean():.2f}\")\n",
    "print(f\"Median tokens: {token_lengths.median():.1f}\")\n",
    "print(f\"Min tokens: {token_lengths.min()}\")\n",
    "print(f\"Max tokens: {token_lengths.max()}\")\n",
    "print(f\"Standard deviation: {token_lengths.std():.2f}\")\n",
    "\n",
    "# Character length analysis\n",
    "char_lengths = descriptions.str.len()\n",
    "print(f\"\\nAverage characters per description: {char_lengths.mean():.2f}\")\n",
    "print(f\"Max characters: {char_lengths.max()}\")\n",
    "\n",
    "# TOP N-GRAMS ANALYSIS\n",
    "print(\"\\n TOP N-GRAMS ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    # Convert to uppercase and remove extra spaces\n",
    "    text = str(text).upper().strip()\n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^A-Z\\s]', ' ', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Clean descriptions\n",
    "cleaned_descriptions = descriptions.apply(clean_text)\n",
    "\n",
    "# Top 1-grams (words)\n",
    "all_words = ' '.join(cleaned_descriptions).split()\n",
    "top_words = Counter(all_words).most_common(15)\n",
    "print(\"Top 15 Words (1-grams):\")\n",
    "for word, count in top_words:\n",
    "    print(f\"  '{word}': {count} times ({count/len(descriptions)*100:.2f}% of descriptions)\")\n",
    "\n",
    "# Top 2-grams\n",
    "vectorizer_2gram = CountVectorizer(ngram_range=(2, 2), max_features=10)\n",
    "try:\n",
    "    vectorizer_2gram.fit(cleaned_descriptions)\n",
    "    feature_names = vectorizer_2gram.get_feature_names_out()\n",
    "    word_count_vector = vectorizer_2gram.transform(cleaned_descriptions)\n",
    "    word_counts = word_count_vector.sum(axis=0).A1\n",
    "    top_2grams = [(feature_names[i], word_counts[i]) for i in word_counts.argsort()[-10:][::-1]]\n",
    "    \n",
    "    print(\"\\nTop 10 2-grams:\")\n",
    "    for phrase, count in top_2grams:\n",
    "        print(f\"  '{phrase}': {count} times\")\n",
    "except:\n",
    "    print(\"\\nCould not generate 2-grams (insufficient data)\")\n",
    "\n",
    "# 3. VOCABULARY ANALYSIS\n",
    "print(\"\\n VOCABULARY ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "unique_words = set(all_words)\n",
    "print(f\"Total unique words (vocabulary size): {len(unique_words)}\")\n",
    "print(f\"Total word instances: {len(all_words)}\")\n",
    "print(f\"Vocabulary richness: {len(unique_words)/len(all_words):.4f}\")\n",
    "\n",
    "# Most frequent word categories (injury-related)\n",
    "injury_words = [word for word in all_words if 'INJUR' in word or 'HURT' in word or 'PAIN' in word]\n",
    "body_parts = [word for word in all_words if any(part in word for part in ['ARM', 'LEG', 'BACK', 'HAND', 'FINGER', 'HEAD', 'NECK', 'SHOULDER'])]\n",
    "actions = [word for word in all_words if any(action in word for action in ['LIFT', 'FALL', 'CUT', 'HIT', 'SLIP', 'TWIST'])]\n",
    "\n",
    "print(f\"\\nDomain-specific word categories:\")\n",
    "print(f\"  Injury-related words: {len(injury_words)} instances\")\n",
    "print(f\"  Body part mentions: {len(body_parts)} instances\")\n",
    "print(f\"  Action words: {len(actions)} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd45cf-3de4-4e80-abcb-e7e4e0b81a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
