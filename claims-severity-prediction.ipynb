{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc065d4a-d06d-4f22-8c35-8e1b261be526",
   "metadata": {},
   "source": [
    "# Claims Severity Prediction by Fine-Tuning a Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba838693-6183-4994-9b78-b5a419c17a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASIC PANDAS PROFILING FOR NUMERIC COLUMNS ===\n",
      "Dataset shape: (54000, 15)\n",
      "\n",
      "Numeric columns found: 8\n",
      "Column names: ['Age', 'DependentChildren', 'DependentsOther', 'WeeklyWages', 'HoursWorkedPerWeek', 'DaysWorkedPerWeek', 'InitialIncurredCalimsCost', 'UltimateIncurredClaimCost']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üìä COLUMN: Age\n",
      "--------------------------------------------------\n",
      "üìà Descriptive Statistics:\n",
      "count    54000.000000\n",
      "mean        33.842370\n",
      "std         12.122165\n",
      "min         13.000000\n",
      "25%         23.000000\n",
      "50%         32.000000\n",
      "75%         43.000000\n",
      "max         81.000000\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "üîç Data Quality:\n",
      "   ‚Ä¢ Null values: 0 (0.00%)\n",
      "   ‚Ä¢ Unique values: 68\n",
      "   ‚Ä¢ Data type: int64\n",
      "   ‚Ä¢ Range: 68.00\n",
      "   ‚Ä¢ Coefficient of Variation: 35.82%\n",
      "   ‚Ä¢ Potential outliers: 22 (0.04%)\n",
      "==================================================\n",
      "\n",
      "üìä COLUMN: DependentChildren\n",
      "--------------------------------------------------\n",
      "üìà Descriptive Statistics:\n",
      "count    54000.000000\n",
      "mean         0.119185\n",
      "std          0.517780\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          9.000000\n",
      "Name: DependentChildren, dtype: float64\n",
      "\n",
      "üîç Data Quality:\n",
      "   ‚Ä¢ Null values: 0 (0.00%)\n",
      "   ‚Ä¢ Unique values: 9\n",
      "   ‚Ä¢ Data type: int64\n",
      "   ‚Ä¢ Range: 9.00\n",
      "   ‚Ä¢ Coefficient of Variation: 434.43%\n",
      "   ‚Ä¢ Potential outliers: 3361 (6.22%)\n",
      "==================================================\n",
      "\n",
      "üìä COLUMN: DependentsOther\n",
      "--------------------------------------------------\n",
      "üìà Descriptive Statistics:\n",
      "count    54000.000000\n",
      "mean         0.009944\n",
      "std          0.109348\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          5.000000\n",
      "Name: DependentsOther, dtype: float64\n",
      "\n",
      "üîç Data Quality:\n",
      "   ‚Ä¢ Null values: 0 (0.00%)\n",
      "   ‚Ä¢ Unique values: 5\n",
      "   ‚Ä¢ Data type: int64\n",
      "   ‚Ä¢ Range: 5.00\n",
      "   ‚Ä¢ Coefficient of Variation: 1099.58%\n",
      "   ‚Ä¢ Potential outliers: 494 (0.91%)\n",
      "==================================================\n",
      "\n",
      "üìä COLUMN: WeeklyWages\n",
      "--------------------------------------------------\n",
      "üìà Descriptive Statistics:\n",
      "count    54000.000000\n",
      "mean       416.364807\n",
      "std        248.638669\n",
      "min          1.000000\n",
      "25%        200.000000\n",
      "50%        392.200000\n",
      "75%        500.000000\n",
      "max       7497.000000\n",
      "Name: WeeklyWages, dtype: float64\n",
      "\n",
      "üîç Data Quality:\n",
      "   ‚Ä¢ Null values: 0 (0.00%)\n",
      "   ‚Ä¢ Unique values: 13211\n",
      "   ‚Ä¢ Data type: float64\n",
      "   ‚Ä¢ Range: 7496.00\n",
      "   ‚Ä¢ Coefficient of Variation: 59.72%\n",
      "   ‚Ä¢ Potential outliers: 1478 (2.74%)\n",
      "==================================================\n",
      "\n",
      "üìä COLUMN: HoursWorkedPerWeek\n",
      "--------------------------------------------------\n",
      "üìà Descriptive Statistics:\n",
      "count    54000.000000\n",
      "mean        37.735084\n",
      "std         12.568704\n",
      "min          0.000000\n",
      "25%         38.000000\n",
      "50%         38.000000\n",
      "75%         40.000000\n",
      "max        640.000000\n",
      "Name: HoursWorkedPerWeek, dtype: float64\n",
      "\n",
      "üîç Data Quality:\n",
      "   ‚Ä¢ Null values: 0 (0.00%)\n",
      "   ‚Ä¢ Unique values: 424\n",
      "   ‚Ä¢ Data type: float64\n",
      "   ‚Ä¢ Range: 640.00\n",
      "   ‚Ä¢ Coefficient of Variation: 33.31%\n",
      "   ‚Ä¢ Potential outliers: 7446 (13.79%)\n",
      "==================================================\n",
      "\n",
      "üìä COLUMN: DaysWorkedPerWeek\n",
      "--------------------------------------------------\n",
      "üìà Descriptive Statistics:\n",
      "count    54000.000000\n",
      "mean         4.905759\n",
      "std          0.552129\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          5.000000\n",
      "75%          5.000000\n",
      "max          7.000000\n",
      "Name: DaysWorkedPerWeek, dtype: float64\n",
      "\n",
      "üîç Data Quality:\n",
      "   ‚Ä¢ Null values: 0 (0.00%)\n",
      "   ‚Ä¢ Unique values: 7\n",
      "   ‚Ä¢ Data type: int64\n",
      "   ‚Ä¢ Range: 6.00\n",
      "   ‚Ä¢ Coefficient of Variation: 11.25%\n",
      "   ‚Ä¢ Potential outliers: 4815 (8.92%)\n",
      "==================================================\n",
      "\n",
      "üìä COLUMN: InitialIncurredCalimsCost\n",
      "--------------------------------------------------\n",
      "üìà Descriptive Statistics:\n",
      "count    5.400000e+04\n",
      "mean     7.841146e+03\n",
      "std      2.058408e+04\n",
      "min      1.000000e+00\n",
      "25%      7.000000e+02\n",
      "50%      2.000000e+03\n",
      "75%      9.500000e+03\n",
      "max      2.000000e+06\n",
      "Name: InitialIncurredCalimsCost, dtype: float64\n",
      "\n",
      "üîç Data Quality:\n",
      "   ‚Ä¢ Null values: 0 (0.00%)\n",
      "   ‚Ä¢ Unique values: 1989\n",
      "   ‚Ä¢ Data type: int64\n",
      "   ‚Ä¢ Range: 1999999.00\n",
      "   ‚Ä¢ Coefficient of Variation: 262.51%\n",
      "   ‚Ä¢ Potential outliers: 4355 (8.06%)\n",
      "==================================================\n",
      "\n",
      "üìä COLUMN: UltimateIncurredClaimCost\n",
      "--------------------------------------------------\n",
      "üìà Descriptive Statistics:\n",
      "count    5.400000e+04\n",
      "mean     1.100337e+04\n",
      "std      3.339099e+04\n",
      "min      1.218868e+02\n",
      "25%      9.263384e+02\n",
      "50%      3.371242e+03\n",
      "75%      8.197249e+03\n",
      "max      4.027136e+06\n",
      "Name: UltimateIncurredClaimCost, dtype: float64\n",
      "\n",
      "üîç Data Quality:\n",
      "   ‚Ä¢ Null values: 0 (0.00%)\n",
      "   ‚Ä¢ Unique values: 53999\n",
      "   ‚Ä¢ Data type: float64\n",
      "   ‚Ä¢ Range: 4027014.05\n",
      "   ‚Ä¢ Coefficient of Variation: 303.46%\n",
      "   ‚Ä¢ Potential outliers: 6805 (12.60%)\n",
      "==================================================\n",
      "\n",
      "‚úÖ Profiling completed for 8 numeric columns!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "print(\"=== BASIC PANDAS PROFILING FOR NUMERIC COLUMNS ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns\n",
    "print(f\"\\nNumeric columns found: {len(numeric_columns)}\")\n",
    "print(f\"Column names: {numeric_columns.tolist()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Profile each numeric column\n",
    "for col in numeric_columns:\n",
    "    print(f\"\\nüìä COLUMN: {col}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"üìà Descriptive Statistics:\")\n",
    "    print(df[col].describe())\n",
    "    \n",
    "    # Data quality\n",
    "    print(f\"\\nüîç Data Quality:\")\n",
    "    print(f\"   ‚Ä¢ Null values: {df[col].isnull().sum()} ({df[col].isnull().sum()/len(df)*100:.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ Unique values: {df[col].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Data type: {df[col].dtype}\")\n",
    "    \n",
    "    # Additional insights\n",
    "    if df[col].nunique() > 1:  # Avoid division by zero\n",
    "        print(f\"   ‚Ä¢ Range: {df[col].max() - df[col].min():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Coefficient of Variation: {df[col].std()/df[col].mean()*100:.2f}%\")\n",
    "    \n",
    "    # Check for potential outliers (using IQR method)\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    print(f\"   ‚Ä¢ Potential outliers: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n‚úÖ Profiling completed for {len(numeric_columns)} numeric columns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa8b7d-08b0-4080-9a06-6dff61636fc9",
   "metadata": {},
   "source": [
    "## Numerical Columns Profiling Analysis\n",
    "\n",
    "### Dataset Overview\n",
    "- **Total Records:** 54,000 \n",
    "- **Numerical Columns:** 8 columns\n",
    "- **Data Quality:** Excellent - Zero missing values across all features\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Analysis of All Numerical Features\n",
    "\n",
    "#### **Demographic Features**\n",
    "\n",
    "**1. Age**\n",
    "- **Mean:** 33.8 years | **Range:** 13-81 years (68 years span)\n",
    "- **Distribution:** Well-balanced (CV: 35.82%)\n",
    "- **Quality:** Minimal outliers (0.04%) - excellent for modeling\n",
    "\n",
    "**2. DependentChildren**\n",
    "- **Mean:** 0.12 children | **Range:** 0-9 children\n",
    "- **Distribution:** Highly skewed - 75% have no children (CV: 434%)\n",
    "- **Outliers:** 6.22% - natural for count data pattern\n",
    "\n",
    "**3. DependentsOther**\n",
    "- **Mean:** 0.01 individuals | **Range:** 0-5 dependents  \n",
    "- **Distribution:** Extremely sparse (CV: 1099%) - 99% have zero\n",
    "- **Recommendation:** Consider removal due to low information value\n",
    "\n",
    "---\n",
    "\n",
    "#### **Employment & Economic Features**\n",
    "\n",
    "**4. WeeklyWages**\n",
    "- **Mean:** $416.36 | **Median:** $392.20 | **Range:** $1-$7,497\n",
    "- **Distribution:** Right-skewed (CV: 59.72%) - typical wage distribution\n",
    "- **Outliers:** 2.74% - high earners, manageable level\n",
    "- **Insights:** Most workers earn $200-$500/week (Q1-Q3)\n",
    "\n",
    "**5. HoursWorkedPerWeek** ‚ö†Ô∏è\n",
    "- **Mean:** 37.7 hours | **Median:** 38 hours | **Range:** 0-640 hours\n",
    "- **Distribution:** Clustered around full-time (CV: 33.31%)\n",
    "- **Critical Issue:** 13.79% outliers - some unrealistic values (640+ hours)\n",
    "- **Action Required:** Cap extreme values or investigate data quality\n",
    "\n",
    "**6. DaysWorkedPerWeek**\n",
    "- **Mean:** 4.9 days | **Median:** 5 days | **Range:** 1-7 days\n",
    "- **Distribution:** Very stable (CV: 11.25%) - mostly standard work week\n",
    "- **Outliers:** 8.92% - likely weekend/shift workers\n",
    "\n",
    "---\n",
    "\n",
    "#### **Claim Cost Features (Critical for Prediction)**\n",
    "\n",
    "**7. InitialIncurredClaimsCost**\n",
    "- **Mean:** $7,841 | **Median:** $2,000 | **Range:** $1-$2M\n",
    "- **Distribution:** Heavily right-skewed (CV: 262.51%)\n",
    "- **Outliers:** 8.06% - high-cost initial assessments\n",
    "- **Pattern:** Mean >> Median indicates extreme skewness\n",
    "\n",
    "**8. UltimateIncurredClaimCost** **[TARGET VARIABLE]**\n",
    "- **Mean:** $11,003 | **Median:** $3,371 | **Range:** $122-$4.03M\n",
    "- **Distribution:** Extremely right-skewed (CV: 303.46%)\n",
    "- **Outliers:** 12.60% - highest among all features\n",
    "- **Critical Insight:** Claims escalate from initial ($7.8K) to ultimate ($11K) on average\n",
    "\n",
    "---\n",
    "\n",
    "### Key Data Insights\n",
    "\n",
    "#### **Distribution Patterns:**\n",
    "- **Normal/Balanced:** Age, DaysWorkedPerWeek\n",
    "- **Right-Skewed:** WeeklyWages, Cost variables\n",
    "- **Highly Sparse:** DependentChildren, DependentsOther\n",
    "- **Clustered:** HoursWorkedPerWeek around 38-40 hours\n",
    "\n",
    "#### **Data Quality Issues:**\n",
    "1. **HoursWorkedPerWeek:** Unrealistic maximum (640 hours) needs investigation\n",
    "2. **Cost Variables:** Extreme outliers but expected in insurance data\n",
    "3. **Dependent Variables:** Very sparse, limited predictive value\n",
    "\n",
    "---\n",
    "\n",
    "### **Comprehensive Preprocessing Strategy**\n",
    "\n",
    "#### **Feature Transformations:**\n",
    "\n",
    "**1. Log Transformation Required:**\n",
    "- WeeklyWages, InitialIncurredClaimsCost, UltimateIncurredClaimCost\n",
    "- *Reason:* Heavy right-skewness (CV > 100%)\n",
    "\n",
    "**2. Outlier Treatment:**\n",
    "- **HoursWorkedPerWeek:** Cap at reasonable maximum (80 hours)\n",
    "- **Cost Variables:** Use robust scaling methods\n",
    "- **Age:** Minimal outliers, keep as-is\n",
    "\n",
    "**3. Feature Engineering:**\n",
    "- **Hourly Rate:** WeeklyWages √∑ HoursWorkedPerWeek\n",
    "- **Cost Escalation:** UltimateIncurredClaimCost √∑ InitialIncurredClaimsCost\n",
    "- **Work Intensity:** Categorical encoding for hours/days patterns\n",
    "\n",
    "**4. Scaling Strategy:**\n",
    "- **StandardScaler:** Age, work pattern features\n",
    "- **RobustScaler:** Cost variables (outlier-resistant)\n",
    "- **LogNormal:** Wage and cost variables after log transformation\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Development Implications**\n",
    "\n",
    "#### **Feature Importance Ranking (Expected):**\n",
    "1. **High Impact:** InitialIncurredClaimsCost, WeeklyWages, Age\n",
    "2. **Medium Impact:** HoursWorkedPerWeek, DaysWorkedPerWeek\n",
    "3. **Low Impact:** DependentChildren, DependentsOther\n",
    "\n",
    "#### **Target Variable Characteristics:**\n",
    "- **Extreme Skewness:** Requires log transformation\n",
    "- **High Outlier Rate:** 12.60% - consider robust loss functions\n",
    "- **Wide Range:** $122 to $4M - multi-scale prediction challenge\n",
    "\n",
    "#### **Correlation Expectations:**\n",
    "- **Strong:** InitialIncurredClaimsCost ‚Üî UltimateIncurredClaimCost\n",
    "- **Moderate:** Age ‚Üî WeeklyWages, WeeklyWages ‚Üî HoursWorkedPerWeek\n",
    "- **Weak:** Dependent variables with other features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360924b-1ff0-4287-a354-f7c75a4779ce",
   "metadata": {},
   "source": [
    "## NLP-specific stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5a51fc4-58d2-46a6-8897-f9804dde9e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NLP-SPECIFIC STATISTICS FOR CLAIM DESCRIPTIONS ===\n",
      "Total descriptions: 54000\n",
      "Empty descriptions: 0\n",
      "\n",
      "üìù TOKEN LENGTH ANALYSIS\n",
      "--------------------------------------------------\n",
      "Average tokens per description: 7.02\n",
      "Median tokens: 7.0\n",
      "Min tokens: 1\n",
      "Max tokens: 14\n",
      "Standard deviation: 1.65\n",
      "\n",
      "Average characters per description: 43.45\n",
      "Max characters: 94\n",
      "\n",
      "üî§ TOP N-GRAMS ANALYSIS\n",
      "--------------------------------------------------\n",
      "Top 15 Words (1-grams):\n",
      "  'RIGHT': 22648 times (41.94% of descriptions)\n",
      "  'LEFT': 20756 times (38.44% of descriptions)\n",
      "  'BACK': 16346 times (30.27% of descriptions)\n",
      "  'STRAIN': 15259 times (28.26% of descriptions)\n",
      "  'LOWER': 9950 times (18.43% of descriptions)\n",
      "  'AND': 9103 times (16.86% of descriptions)\n",
      "  'FINGER': 8584 times (15.90% of descriptions)\n",
      "  'LIFTING': 8300 times (15.37% of descriptions)\n",
      "  'HAND': 7723 times (14.30% of descriptions)\n",
      "  'STRUCK': 7354 times (13.62% of descriptions)\n",
      "  'SHOULDER': 6198 times (11.48% of descriptions)\n",
      "  'FELL': 5747 times (10.64% of descriptions)\n",
      "  'SLIPPED': 5640 times (10.44% of descriptions)\n",
      "  'LACERATION': 5480 times (10.15% of descriptions)\n",
      "  'EYE': 5343 times (9.89% of descriptions)\n",
      "\n",
      "Top 10 2-grams:\n",
      "  'lower back': 9442 times\n",
      "  'back strain': 6374 times\n",
      "  'foreign body': 3880 times\n",
      "  'strain lower': 2981 times\n",
      "  'left hand': 2747 times\n",
      "  'right shoulder': 2609 times\n",
      "  'index finger': 2571 times\n",
      "  'right hand': 2555 times\n",
      "  'laceration left': 2333 times\n",
      "  'right knee': 2193 times\n",
      "\n",
      "üìö VOCABULARY ANALYSIS\n",
      "--------------------------------------------------\n",
      "Total unique words (vocabulary size): 3726\n",
      "Total word instances: 379048\n",
      "Vocabulary richness: 0.0098\n",
      "\n",
      "Domain-specific word categories:\n",
      "  Injury-related words: 7336 instances\n",
      "  Body part mentions: 50753 instances\n",
      "  Action words: 24771 instances\n",
      "\n",
      "‚öñÔ∏è LABEL SKEW ANALYSIS\n",
      "--------------------------------------------------\n",
      "Cost Distribution Analysis:\n",
      "\n",
      "Equal-width binning:\n",
      "  Very Low (<$1K): 26.4% (14,279 claims)\n",
      "  Low ($1K-$5K): 32.9% (17,775 claims)\n",
      "  Medium ($5K-$10K): 19.9% (10,755 claims)\n",
      "  High ($10K-$50K): 16.2% (8,754 claims)\n",
      "  Very High (>$50K): 4.5% (2,437 claims)\n",
      "\n",
      "Quantile-based analysis:\n",
      "  0-25th percentile: $0 - $926\n",
      "  25-50th percentile: $926 - $3,371\n",
      "  50-75th percentile: $3,371 - $8,197\n",
      "  75-90th percentile: $8,197 - $24,006\n",
      "  90-100th percentile: $24,006 - $4,027,136\n",
      "\n",
      "Target variable skewness: 37.55\n",
      "  ‚Üí Highly right-skewed (log transformation recommended)\n",
      "\n",
      "‚úÖ TEXT QUALITY METRICS\n",
      "--------------------------------------------------\n",
      "Very short descriptions (‚â§2 tokens): 112 (0.21%)\n",
      "Descriptions containing numbers: 0 (0.00%)\n",
      "All uppercase descriptions: 54000 (100.00%)\n",
      "\n",
      "‚úÖ NLP Analysis completed!\n",
      "Recommendations:\n",
      "  ‚Ä¢ Text is clean and consistent (all uppercase)\n",
      "  ‚Ä¢ Good description length for model training\n",
      "  ‚Ä¢ Strong domain vocabulary for injury claims\n",
      "  ‚Ä¢ Target variable needs log transformation due to skewness\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "descriptions = df['ClaimDescription'].fillna('')\n",
    "\n",
    "print(\"=== NLP-SPECIFIC STATISTICS FOR CLAIM DESCRIPTIONS ===\")\n",
    "print(f\"Total descriptions: {len(descriptions)}\")\n",
    "print(f\"Empty descriptions: {descriptions.str.strip().eq('').sum()}\")\n",
    "\n",
    "# 1. TOKEN LENGTH ANALYSIS\n",
    "print(\"\\nüìù TOKEN LENGTH ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "token_lengths = descriptions.str.split().str.len()\n",
    "print(f\"Average tokens per description: {token_lengths.mean():.2f}\")\n",
    "print(f\"Median tokens: {token_lengths.median():.1f}\")\n",
    "print(f\"Min tokens: {token_lengths.min()}\")\n",
    "print(f\"Max tokens: {token_lengths.max()}\")\n",
    "print(f\"Standard deviation: {token_lengths.std():.2f}\")\n",
    "\n",
    "# Character length analysis\n",
    "char_lengths = descriptions.str.len()\n",
    "print(f\"\\nAverage characters per description: {char_lengths.mean():.2f}\")\n",
    "print(f\"Max characters: {char_lengths.max()}\")\n",
    "\n",
    "# 2. TOP N-GRAMS ANALYSIS\n",
    "print(\"\\nüî§ TOP N-GRAMS ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    # Convert to uppercase and remove extra spaces\n",
    "    text = str(text).upper().strip()\n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^A-Z\\s]', ' ', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Clean descriptions\n",
    "cleaned_descriptions = descriptions.apply(clean_text)\n",
    "\n",
    "# Top 1-grams (words)\n",
    "all_words = ' '.join(cleaned_descriptions).split()\n",
    "top_words = Counter(all_words).most_common(15)\n",
    "print(\"Top 15 Words (1-grams):\")\n",
    "for word, count in top_words:\n",
    "    print(f\"  '{word}': {count} times ({count/len(descriptions)*100:.2f}% of descriptions)\")\n",
    "\n",
    "# Top 2-grams\n",
    "vectorizer_2gram = CountVectorizer(ngram_range=(2, 2), max_features=10)\n",
    "try:\n",
    "    vectorizer_2gram.fit(cleaned_descriptions)\n",
    "    feature_names = vectorizer_2gram.get_feature_names_out()\n",
    "    word_count_vector = vectorizer_2gram.transform(cleaned_descriptions)\n",
    "    word_counts = word_count_vector.sum(axis=0).A1\n",
    "    top_2grams = [(feature_names[i], word_counts[i]) for i in word_counts.argsort()[-10:][::-1]]\n",
    "    \n",
    "    print(\"\\nTop 10 2-grams:\")\n",
    "    for phrase, count in top_2grams:\n",
    "        print(f\"  '{phrase}': {count} times\")\n",
    "except:\n",
    "    print(\"\\nCould not generate 2-grams (insufficient data)\")\n",
    "\n",
    "# 3. VOCABULARY ANALYSIS\n",
    "print(\"\\nüìö VOCABULARY ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "unique_words = set(all_words)\n",
    "print(f\"Total unique words (vocabulary size): {len(unique_words)}\")\n",
    "print(f\"Total word instances: {len(all_words)}\")\n",
    "print(f\"Vocabulary richness: {len(unique_words)/len(all_words):.4f}\")\n",
    "\n",
    "# Most frequent word categories (injury-related)\n",
    "injury_words = [word for word in all_words if 'INJUR' in word or 'HURT' in word or 'PAIN' in word]\n",
    "body_parts = [word for word in all_words if any(part in word for part in ['ARM', 'LEG', 'BACK', 'HAND', 'FINGER', 'HEAD', 'NECK', 'SHOULDER'])]\n",
    "actions = [word for word in all_words if any(action in word for action in ['LIFT', 'FALL', 'CUT', 'HIT', 'SLIP', 'TWIST'])]\n",
    "\n",
    "print(f\"\\nDomain-specific word categories:\")\n",
    "print(f\"  Injury-related words: {len(injury_words)} instances\")\n",
    "print(f\"  Body part mentions: {len(body_parts)} instances\")\n",
    "print(f\"  Action words: {len(actions)} instances\")\n",
    "\n",
    "# 4. LABEL SKEW ANALYSIS (TARGET VARIABLE)\n",
    "print(\"\\n‚öñÔ∏è LABEL SKEW ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "target = df['UltimateIncurredClaimCost']\n",
    "\n",
    "# Multiple binning strategies\n",
    "print(\"Cost Distribution Analysis:\")\n",
    "\n",
    "# Strategy 1: Equal-width bins\n",
    "bins_equal = [0, 1000, 5000, 10000, 50000, float('inf')]\n",
    "labels_equal = ['Very Low (<$1K)', 'Low ($1K-$5K)', 'Medium ($5K-$10K)', 'High ($10K-$50K)', 'Very High (>$50K)']\n",
    "cost_ranges_equal = pd.cut(target, bins=bins_equal, labels=labels_equal)\n",
    "print(\"\\nEqual-width binning:\")\n",
    "distribution = cost_ranges_equal.value_counts(normalize=True).sort_index()\n",
    "for label, pct in distribution.items():\n",
    "    count = cost_ranges_equal.value_counts().sort_index()[label]\n",
    "    print(f\"  {label}: {pct:.1%} ({count:,} claims)\")\n",
    "\n",
    "# Strategy 2: Quantile-based bins\n",
    "quantiles = target.quantile([0, 0.25, 0.5, 0.75, 0.9, 1.0])\n",
    "print(f\"\\nQuantile-based analysis:\")\n",
    "print(f\"  0-25th percentile: $0 - ${quantiles[0.25]:,.0f}\")\n",
    "print(f\"  25-50th percentile: ${quantiles[0.25]:,.0f} - ${quantiles[0.5]:,.0f}\")\n",
    "print(f\"  50-75th percentile: ${quantiles[0.5]:,.0f} - ${quantiles[0.75]:,.0f}\")\n",
    "print(f\"  75-90th percentile: ${quantiles[0.75]:,.0f} - ${quantiles[0.9]:,.0f}\")\n",
    "print(f\"  90-100th percentile: ${quantiles[0.9]:,.0f} - ${quantiles[1.0]:,.0f}\")\n",
    "\n",
    "# Skewness analysis\n",
    "from scipy import stats\n",
    "skewness = stats.skew(target)\n",
    "print(f\"\\nTarget variable skewness: {skewness:.2f}\")\n",
    "if skewness > 1:\n",
    "    print(\"  ‚Üí Highly right-skewed (log transformation recommended)\")\n",
    "elif skewness > 0.5:\n",
    "    print(\"  ‚Üí Moderately right-skewed\")\n",
    "else:\n",
    "    print(\"  ‚Üí Relatively symmetric\")\n",
    "\n",
    "# 5. TEXT QUALITY METRICS\n",
    "print(\"\\n‚úÖ TEXT QUALITY METRICS\")\n",
    "print(\"-\" * 50)\n",
    "# Empty or very short descriptions\n",
    "very_short = (token_lengths <= 2).sum()\n",
    "print(f\"Very short descriptions (‚â§2 tokens): {very_short} ({very_short/len(descriptions)*100:.2f}%)\")\n",
    "\n",
    "# Descriptions with numbers (might indicate codes)\n",
    "with_numbers = descriptions.str.contains(r'\\d', na=False).sum()\n",
    "print(f\"Descriptions containing numbers: {with_numbers} ({with_numbers/len(descriptions)*100:.2f}%)\")\n",
    "\n",
    "# All caps descriptions (current format)\n",
    "all_caps = descriptions.str.isupper().sum()\n",
    "print(f\"All uppercase descriptions: {all_caps} ({all_caps/len(descriptions)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ NLP Analysis completed!\")\n",
    "print(\"Recommendations:\")\n",
    "print(\"  ‚Ä¢ Text is clean and consistent (all uppercase)\")\n",
    "print(\"  ‚Ä¢ Good description length for model training\")\n",
    "print(\"  ‚Ä¢ Strong domain vocabulary for injury claims\")\n",
    "print(\"  ‚Ä¢ Target variable needs log transformation due to skewness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd45cf-3de4-4e80-abcb-e7e4e0b81a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
